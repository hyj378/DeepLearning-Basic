
## 딥러닝의 기본 
모두의 딥러닝의 시작, 홍콩과기대 김성훈 교수님의 딥러닝의 기본강의로 시작합니다.
모임 참석자 모두 해당 강의와 실습을 수행 후 모임에 참석합니다.
각자 자신이 맡은 부분을 주도적으로 세미나를 진행하며, 질의 응답을 받습니다.

## 반 배정
- A반: 안형조, 조원, 한대찬*, 박미희, 권석준, 김해린*, 노현중, 곽민재 
- B반: 박진현, 황유진, 최영민, 김민준, 김남훈*, 조동현, 이정민, 이영남, 변성진, 김지원*
- *각반 임시 반장들입니다. 반장 여러분의 적극 협조 부탁드립니다.


----

## (1주차) A반: 11월 12일 / B반: 11월 15일

- 섹션 0. 오리엔테이션 (송인재, 박진현)
    - 수업 소개와 개요	[비디오](https://www.youtube.com/watch?v=BS6O0zOGX4E)/[슬라이드](https://hunkim.github.io/ml/lec0.pdf) [00:10:00]

- 섹션 1. 머신러닝의 개념과 용어
    - 기본적인 Machine Learnnig 의 용어와 개념 설명	[비디오](https://www.youtube.com/watch?v=qPMeuL2LIqY)/[슬라이드](https://hunkim.github.io/ml/lec1.pdf) [00:12:00] (정지원, 황유진)
    - TensorFlow의 설치및 기본적인 operations [비디오](https://youtu.be/-57Ne86Ia8w)/[슬라이드](https://docs.google.com/presentation/d/137IlT2N3AYcclqxNuc8j9RDrIeHiYkSZ5JPg_vg9Jqk/edit#slide=id.g1d115b0ec5_0_215) [00:17:00] (안형조, 최영민)

- 섹션 2. Linear Regression 의 개념
    - Linear Regression의 Hypothesis 와 cost [비디오](https://www.youtube.com/watch?v=Hax03rCn3UI)[슬라이드](https://hunkim.github.io/ml/lec2.pdf) [00:13:00] (조원, 김민준)
    - Tensorflow로 간단한 linear regression을 구현 [비디오](https://youtu.be/mQGwjrStQgg)/[슬라이드](https://docs.google.com/presentation/d/12raZrY3d244q6jGuC7EykeSPzjP1-FqofMiNlx5Q52o) [00:15:00] (한대찬, 김남훈)

- 섹션 3. Linear Regression cost 함수 최소화 
    - Linear Regression의 cost 최소화 알고리즘의 원리	[비디오](https://www.youtube.com/watch?v=TxIVr-nk1so)/[슬라이드](https://hunkim.github.io/ml/lec3.pdf) [00:16:00] (박미희, 조동현)
    - Linear Regression 의 cost 최소화의 TensorFlow 구현 [비디오](https://youtu.be/Y0EF9VqRuEA)/[슬라이드](https://docs.google.com/presentation/d/1Az_ulisKyBH7hVNrQmN_3HyrX1sAxUMqXQvvtaRGYl4) [00:15:00] (권석준, 이정민)

- 섹션 4. 여러개의 입력(feature)의 Linear Regression
    - multi-variable linear regression [비디오](https://youtu.be/kPxpJY6fRkY)/[슬라이드](https://docs.google.com/presentation/d/1bHVxjCVvRKjCgtf6OMmxe35nR65LnsERoWSefWscv2I/) [00:17:00] (김해린, 이영남)
    - lab 04-1: multi-variable linear regression을 TensorFlow에서 구현하기	[비디오](https://youtu.be/fZUV3xjoZSM)/[슬라이드](https://docs.google.com/presentation/d/1WF5yphSXyzYLG8wmVvOpRmgAlw4vewbK51ZwLAOFZXk) [00:08:00] (노현중, 변성진)
    - lab 04-2: TensorFlow로 파일에서 데이타 읽어오기 [비디오](https://youtu.be/o2q4QNnoShY) [00:06:00] (곽민재, 김지원)

- 1주차 Summary
    - (김재형, 박진현)

## (2주차) A반: 11월 26일 / B반: 11월 29일

- 섹션 5. Logistic (Regression) Classification 
    - Logistic Classification의 가설 함수 정의	[비디오](https://youtu.be/PIjno6paszY)/[슬라이드](https://hunkim.github.io/ml/lec5.pdf)	[00:15:00] (송인재, 박진현)
    - Logistic Regression의 cost 함수 설명 [비디오](https://youtu.be/6vzchGYEJBc)/[슬라이드](https://hunkim.github.io/ml/lec5.pdf)		[00:14:00] (정지원, 황유진)
    - TensorFlow로 Logistic Classification의 구현하기 [비디오](https://youtu.be/2FeWGgnyLSw)/[슬라이드](https://docs.google.com/presentation/d/180ZISPNRVWYKyV61xoZepZ_KVUK6mujIXuwXE0eKZuM)		[00:15:00] (안형조, 최영민)


- 섹션 6. Softmax Regression (Multinomial Logistic Regression)
    - Multinomial 개념 소개	[비디오](https://youtu.be/MFAnsx1y9ZI)/[슬라이드](https://hunkim.github.io/ml/lec6.pdf) [00:10:00] (조원, 김민준)
    - Cost 함수 소개	[비디오](https://youtu.be/jMU9G5WEtBc)/[슬라이드](https://hunkim.github.io/ml/lec6.pdf) [00:15:00] (한대찬, 김남훈)
    - lab 06-1: TensorFlow로 Softmax Classification의 구현하기 [비디오](https://youtu.be/VRnubDzIy3A)/[슬라이드](https://docs.google.com/presentation/d/1FPcmOh_gmBw7uyOThFyKwdx7Ua2q8tX0kVFOSwI6kas)	[00:12:00] (박미희, 조동현)
    - lab 06-2: TensorFlow로 Fancy Softmax Classification의 구현하기 [비디오](https://youtu.be/E-io76NlsqA)/[슬라이드](https://docs.google.com/presentation/d/1FPcmOh_gmBw7uyOThFyKwdx7Ua2q8tX0kVFOSwI6kas)		[00:16:00] (권석준, 이정민)


- 섹션 7. ML의 실용과 몇가지 팁
    - 학습 rate, Overfitting, 그리고 일반화 (Regularization)	[비디오](https://youtu.be/1jPjVoDV_uo)/[슬라이드](https://hunkim.github.io/ml/lec7.pdf)	[00:14:00] (김해린, 이영남)
    - Training/Testing 데이타 셋		[비디오](https://youtu.be/KVv1nMSlPzY)/[슬라이드](https://hunkim.github.io/ml/lec7.pdf)[00:09:00] (노현중, 변성진)
    - lab 07-1: training/test dataset, learning rate, normalization [비디오](https://youtu.be/oSJfejG2C3w)/[슬라이드](https://docs.google.com/presentation/d/1cVwqMpERToATs1JGYps0F3MLARP8OAlw6ZIe-lpPHYs)		[00:11:00] (곽민재, 김지원)
    - lab 07-2: Meet MNIST Dataset [비디오](https://youtu.be/ktd5yrki_KA)/[슬라이드](https://docs.google.com/presentation/d/1cVwqMpERToATs1JGYps0F3MLARP8OAlw6ZIe-lpPHYs) [00:13:00] (송인재, 박진현) 


## (3,4,5주차 통합) 12월 26일 6시 스터디 진행


- 섹션 8. 딥러닝의 기본 개념과, 문제, 그리고 해결 
    - 딥러닝의 기본 개념: 시작과 XOR 문제 [비디오](https://youtu.be/n7DNueHGkqE)/[슬라이드](https://hunkim.github.io/ml/lec8.pdf) [00:17:00] 
    - 딥러닝의 기본 개념2: Back-propagation 과 2006/2007 ‘딥’의 출현 [비디오](https://youtu.be/AByVbUX1PUI)/[슬라이드](https://hunkim.github.io/ml/lec8.pdf) [00:12:00] 
   
   ------------------------------------------------------- 곽민재
    - Lab : Tensor Manipulation [비디오](https://youtu.be/ZYX0FaqUeN4)/[슬라이드](https://docs.google.com/presentation/d/1gQ7Xxrhylkr5Kk5pG15yvX3yOln_hk2-H6jrQeXqKmU) [00:26:00]

- 섹션 9. Neural Network 1: XOR 문제와 학습방법, Backpropagation
    - XOR 문제 딥러닝으로 풀기 [비디오](https://youtu.be/GYecDQQwTdI)/[슬라이드](https://hunkim.github.io/ml/lec9.pdf)[00:15:00]
        
        ------------------------------------------------------- 권석준
    - 특별편: 10분안에 미분 정리하기 [비디오](https://youtu.be/oZyvmtqLmLo)/[슬라이드](https://hunkim.github.io/ml/lec9.pdf)[00:09:00]
        
        ------------------------------------------------------- 생략
    - 딥넷트웍 학습 시키기 (backpropagation)	[비디오](https://youtu.be/573EZkzfnZ0)/[슬라이드](https://hunkim.github.io/ml/lec9.pdf)[00:18:00] 
    - Lab 9-1: XOR을 위한 텐스플로우 딥넷트웍 [비디오](https://youtu.be/oFGHOsAYiz0)/[슬라이드](https://docs.google.com/presentation/d/1KHpjyziDm0Wle-OI-6TZhWM2Oj7YiypXuZOZ1SJW8ds/edit?usp=drive_web)	[00:12:00] 
       
       ------------------------------------------------------- 김민준
    - Lab 9-2: Tensor Board로 딥네트웍 들여다보기 [비디오](https://youtu.be/lmrWZPFYjHM)/[슬라이드](https://docs.google.com/presentation/d/1KHpjyziDm0Wle-OI-6TZhWM2Oj7YiypXuZOZ1SJW8ds/edit?usp=drive_web)		[00:12:00]

- 섹션 10. Neural Network 2: ReLU and 초기값 정하기 (2006/2007 breakthrough)
    - XSigmoid 보다 ReLU가 더 좋아 [비디오]()/[슬라이드]()[00:17:00]
      
      ------------------------------------------------------- 노현중
    - Weight 초기화 잘해보자 [비디오]()/[슬라이드]()[00:12:00]
    - Dropout 과 앙상블	[비디오]()/[슬라이드]()[00:10:00]
      
      ------------------------------------------------------- 박미희
    - 레고처렴 넷트웍 모듈을 마음껏 쌓아 보자	[비디오]()/[슬라이드]()	[00:05:00]
    - Lab 10: 딥러닝으로 MNIST 98%이상 해보기 [비디오]()/[슬라이드]()		[00:14:00]
       
       ------------------------------------------------------- 박진현

- 섹션 11. Convolutional Neural Networks
    - ConvNet의 Conv 레이어 만들기	미리보기	[비디오]()/[슬라이드]()[00:16:00]
    - ConvNet Max pooling 과 Full Network	[비디오]()/[슬라이드]() [00:05:00]
      
      ------------------------------------------------------- 변성진
    - ConvNet의 활용 예	[비디오]()/[슬라이드]()	[00:12:00]
    - 실습1: TensorFlow CNN 의 기본	[비디오]()/[슬라이드]()	[00:16:00]
      
      ------------------------------------------------------- 안형조(독감으로 인한 결시 -> 대체자 : 김지원)
    - 실습2: TensorFlow로 구현하자 (MNIST 99%)	[비디오]()/[슬라이드]() [00:12:00]
    - 실습3: Class, tf.layers, Ensemble (MNIST 99.5%)	[비디오]()/[슬라이드]()	[00:10:00]
   
   ------------------------------------------------------- 이영남
- 섹션 12. Recurrent Neural Network
    - NN의 꽃 RNN 이야기	[비디오]()/[슬라이드]() [00:19:00]
    - Lab 12-1 RNN – Basic [비디오]()/[슬라이드]()	[00:12:00]
       
       ------------------------------------------------------- 이정민
    - Lab 12-2 RNN – Hi Hello Training [비디오]()/[슬라이드]()	[00:15:00]
    - Lab 12-3 : Long Sequence RNN [비디오]()/[슬라이드]()	[00:00:00]
      
      ------------------------------------------------------- 조동현
    - Lab12-4: Stacked RNN + Softmax Layer [비디오]()/[슬라이드]()	[00:11:00]
    - Lab12-5: Dynamic RNN [비디오]()/[슬라이드]()	[00:04:00]
     
     ------------------------------------------------------- 조원
    - Lab12-6: RNN with Time Series Data [비디오]()/[슬라이드]() [00:10:00]


- 섹션 13. Deep Deep Network AWS 에서 GPU와 돌려보기 (powered bt AWS)
    - powered by AWS	[비디오]()/[슬라이드]() [00:18:00]

   
   ------------------------------------------------------- 최영민
- 섹션 14. AWS 에서 저렴하게 Spot Instance 를 터미네이션 걱정없이 사용하기
    - AWS에서 저렴하게 Spot Instance를 터미네이션 걱정없이 사용하기	[비디오]()/[슬라이드]() [00:18:00]

- 섹션 15. Google Cloud ML을 이용해 TensorFlow 실행하기
    - Google Cloud ML with Examples 1 [비디오]()/[슬라이드]()
    
    
        ------------------------------------------------------- 


